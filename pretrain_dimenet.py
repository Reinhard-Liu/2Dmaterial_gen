import os
import torch
import lmdb
import pickle
import numpy as np
from torch.utils.data import Dataset
from torch_geometric.loader import DataLoader
from torch_geometric.data import Data
from tqdm import tqdm

# å¼•å…¥æˆ‘ä»¬åœ¨æ¨¡å‹å±‚å®šä¹‰çš„ DimeNet++ =

from models.dimnet_model import build_dimenet_plus_plus


# ==========================================
# 1. å®šä¹‰ OCP LMDB æ•°æ®é›†è¯»å–å™¨ (é€‚é… PyG)
# ==========================================
class OCPLmdbDataset(Dataset):
    def __init__(self, lmdb_path):
        super().__init__()
        self.env = lmdb.open(lmdb_path, subdir=False, readonly=True, lock=False, readahead=False, meminit=False)
        with self.env.begin() as txn:
            self.length = txn.stat()['entries']
            self.keys = [key for key, _ in txn.cursor()]

    def __len__(self):
        return self.length

    def __getitem__(self, idx):
        with self.env.begin() as txn:
            data_bytes = txn.get(self.keys[idx])
            data_obj = pickle.loads(data_bytes)

        # 1. ç»•è¿‡ PyG ç‰ˆæœ¬é™åˆ¶
        data_dict = data_obj.__dict__
        if '_store' in data_dict:
            source_dict = data_dict['_store']
        else:
            source_dict = data_dict

        # 2. æå–åŸå­åºæ•°å’Œåæ ‡ (é€‚é…ä¸åŒç‰ˆæœ¬çš„ OCP)
        raw_z = source_dict.get('atomic_numbers', source_dict.get('z'))
        raw_pos = source_dict['pos']

        # 3. æ™ºèƒ½å¯»æ‰¾ç›®æ ‡èƒ½é‡æ ‡ç­¾ (é€‚é… IS2RE ä»»åŠ¡çš„ y_relaxed)
        if 'y_relaxed' in source_dict:
            raw_y = source_dict['y_relaxed']
        elif 'y' in source_dict:
            raw_y = source_dict['y']
        else:
            # å¦‚æœè¿˜æ˜¯æ‰¾ä¸åˆ°ï¼Œæ‰“å°å‡ºæ‰€æœ‰å¯ç”¨çš„é”®ï¼Œæ–¹ä¾¿æˆ‘ä»¬æ’æŸ¥
            raise KeyError(f"æ•°æ®é›†ä¸­æ‰¾ä¸åˆ°ç›®æ ‡èƒ½é‡æ ‡ç­¾ã€‚å½“å‰å¯ç”¨çš„é”®æœ‰: {list(source_dict.keys())}")

        # 4. å®‰å…¨åœ°è½¬æ¢ä¸º Tensor
        def safe_tensor(val, dtype):
            if isinstance(val, torch.Tensor):
                return val.clone().detach().to(dtype)
            return torch.tensor(val, dtype=dtype)

        z = safe_tensor(raw_z, torch.long)
        pos = safe_tensor(raw_pos, torch.float)
        y = safe_tensor(raw_y, torch.float)

        # ç¡®ä¿å¸é™„èƒ½ y æ˜¯ä¸€ä¸ª 1D Tensor
        if y.dim() == 0:
            y = y.unsqueeze(0)
        elif y.numel() > 1:
            y = y.view(-1)[0].unsqueeze(0)

        return Data(z=z, pos=pos, y=y)


# ==========================================
# 2. æ ¸å¿ƒè®­ç»ƒä¸éªŒè¯å¾ªç¯
# ==========================================
def train_dimenet():
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"Using device: {device}")

    # è¶…å‚æ•°è®¾ç½®
    batch_size = 32
    epochs = 10
    learning_rate = 1e-4
    # ä½¿ç”¨ r'' é˜²æ­¢ Windows è·¯å¾„åæ–œæ è½¬ä¹‰
    lmdb_train_path = r'D:\Programming Software\github_project\MachineLearning_MG\dataset\is2re\10k\train\data.lmdb'
    save_dir = 'models/weights/'
    os.makedirs(save_dir, exist_ok=True)

    # åŠ è½½æ•°æ®
    print("Loading OCP Dataset...")
    train_dataset = OCPLmdbDataset(lmdb_train_path)
    # å®é™…åº”ç”¨ä¸­éœ€è¦åˆ’åˆ† train å’Œ valï¼Œè¿™é‡Œä¸ºæ¼”ç¤ºç®€åŒ–
    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)

    # åˆå§‹åŒ–æ¨¡å‹ä¸ä¼˜åŒ–å™¨
    model = build_dimenet_plus_plus(device)
    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)

    # èƒ½é‡é¢„æµ‹é€šå¸¸ä½¿ç”¨ L1 Loss (MAE) ä½œä¸ºæ ‡å‡†è¯„ä¼°æŒ‡æ ‡
    criterion = torch.nn.L1Loss()

    best_loss = float('inf')

    print("Starting Pre-training...")
    for epoch in range(epochs):
        model.train()
        total_loss = 0.0

        # ä½¿ç”¨ tqdm æ˜¾ç¤ºè¿›åº¦æ¡
        pbar = tqdm(train_loader, desc=f"Epoch {epoch + 1}/{epochs}")
        for batch in pbar:
            batch = batch.to(device)
            optimizer.zero_grad()

            # DimeNet++ å‰å‘ä¼ æ’­ (ä¼ å…¥åŸå­åºæ•°, åæ ‡, å’Œ batch ç´¢å¼•)
            preds = model(batch.z, batch.pos, batch.batch)

            # è®¡ç®—æŸå¤± (é¢„æµ‹å¸é™„èƒ½ vs çœŸå®å¸é™„èƒ½)
            loss = criterion(preds.squeeze(), batch.y)

            loss.backward()
            optimizer.step()

            total_loss += loss.item() * batch.num_graphs
            pbar.set_postfix({'MAE Loss (eV)': f"{loss.item():.4f}"})

        avg_loss = total_loss / len(train_dataset)
        print(f"Epoch {epoch + 1} Completed | Average Train MAE: {avg_loss:.4f} eV")

        # ==========================================
        # 3. ä¿å­˜æœ€ä¼˜æ¨¡å‹æƒé‡
        # ==========================================
        if avg_loss < best_loss:
            best_loss = avg_loss
            save_path = os.path.join(save_dir, 'dimenet_best_ocp.pth')

            # æ¨èä¿å­˜ state_dict è€Œä¸æ˜¯æ•´ä¸ªæ¨¡å‹ï¼Œè¿™æ ·æ›´çµæ´»ä¸”è·¨ç‰ˆæœ¬å…¼å®¹
            torch.save({
                'epoch': epoch,
                'model_state_dict': model.state_dict(),
                'optimizer_state_dict': optimizer.state_dict(),
                'loss': best_loss,
            }, save_path)
            print(f"ğŸŒŸ æ–°çš„æœ€ä¼˜æƒé‡å·²ä¿å­˜è‡³: {save_path}")


if __name__ == "__main__":
    train_dimenet()