import torch
import torch.nn as nn
from torch_scatter import scatter_sum, scatter_mean
import math


# ==========================================
# 1. åŸºç¡€ç»„ä»¶ï¼šæ­£å¼¦æ—¶é—´æ­¥åµŒå…¥
# ==========================================
class SinusoidalTimeEmbeddings(nn.Module):
    def __init__(self, dim):
        super().__init__()
        self.dim = dim

    def forward(self, time):
        device = time.device
        half_dim = self.dim // 2
        embeddings = math.log(10000) / (half_dim - 1)
        embeddings = torch.exp(torch.arange(half_dim, device=device) * -embeddings)
        embeddings = time[:, None] * embeddings[None, :]
        embeddings = torch.cat((embeddings.sin(), embeddings.cos()), dim=-1)
        return embeddings


# ==========================================
# 2. æ ¸å¿ƒï¼šE(3) ç­‰å˜å›¾ç½‘ç»œå±‚ (EGNN Layer)
# ==========================================
class EGNNLayer(nn.Module):
    def __init__(self, hidden_dim, edge_dim=0, act_fn=nn.SiLU()):
        super().__init__()
        self.hidden_dim = hidden_dim

        self.edge_mlp = nn.Sequential(
            nn.Linear(hidden_dim * 2 + 1 + edge_dim, hidden_dim), act_fn,
            nn.Linear(hidden_dim, hidden_dim), act_fn
        )
        self.coord_mlp = nn.Sequential(
            nn.Linear(hidden_dim, hidden_dim), act_fn,
            nn.Linear(hidden_dim, 1, bias=False)
        )
        # ðŸš¨ [å…³é”®ä¿®å¤ 1]ï¼šå°†åæ ‡æ›´æ–°çš„æœ€åŽä¸€å±‚æƒé‡å¼ºåˆ¶åˆå§‹åŒ–ä¸º 0ã€‚
        # è¿™ç¡®ä¿äº†æ¨¡åž‹åœ¨æœ€å¼€å§‹ä¸ä¼šä¹±åŠ¨åŽŸå­ï¼Œè€Œæ˜¯ä»Ž 0 å¼€å§‹å¹³ç¨³å­¦ä¹ 
        nn.init.zeros_(self.coord_mlp[2].weight)

        self.node_mlp = nn.Sequential(
            nn.Linear(hidden_dim * 2, hidden_dim), act_fn,
            nn.Linear(hidden_dim, hidden_dim)
        )

    def forward(self, h, x, edge_index, edge_attr=None):
        row, col = edge_index
        coord_diff = x[row] - x[col]

        # ðŸš¨ [å…³é”®ä¿®å¤ 2]ï¼šå¯¹å¹³æ–¹è·ç¦»åš max æˆªæ–­ (æ¯”å¦‚ 100 åŸƒå¹³æ–¹)
        # é˜²æ­¢å‘¨æœŸæ€§è¾¹ç•Œè·¨è¶Šå¯¼è‡´çš„å¼‚å¸¸å¤§è·ç¦»ç‚¸æ¯ç½‘ç»œ
        radial = torch.sum(coord_diff ** 2, 1).unsqueeze(1)
        radial = torch.clamp(radial, max=100.0)

        # ... åŽç»­ä»£ç ä¿æŒä¸å˜ ...

        if edge_attr is not None:
            edge_inputs = torch.cat([h[row], h[col], radial, edge_attr], dim=1)
        else:
            edge_inputs = torch.cat([h[row], h[col], radial], dim=1)

        edge_messages = self.edge_mlp(edge_inputs)

        coord_weights = self.coord_mlp(edge_messages)
        coord_updates = coord_diff * coord_weights

        coord_update_sum = scatter_sum(coord_updates, row, dim=0, dim_size=x.size(0))
        x_new = x + coord_update_sum

        node_messages = scatter_sum(edge_messages, row, dim=0, dim_size=h.size(0))
        node_inputs = torch.cat([h, node_messages], dim=1)
        h_new = h + self.node_mlp(node_inputs)

        return h_new, x_new


# ==========================================
# 3. 2D ææ–™æ‰©æ•£ä¸»æ¨¡åž‹ (æ”¯æŒ CFG æ¡ä»¶å¼•å¯¼)
# ==========================================
class DenoisingEGNN(nn.Module):
    def __init__(self, num_node_features, hidden_dim, num_layers=4):
        super().__init__()
        self.hidden_dim = hidden_dim

        # ðŸš¨ [å…³é”®ä¿®å¤ 3]ï¼šä½¿ç”¨ Embedding æ›¿ä»£ Linear å¤„ç†åŽŸå­åºæ•° (å‡è®¾æœ€å¤§å…ƒç´ å·ä¸º 100)
        self.node_embed = nn.Embedding(100, hidden_dim)
        self.time_embed = SinusoidalTimeEmbeddings(hidden_dim)

        self.time_mlp = nn.Sequential(
            nn.Linear(hidden_dim, hidden_dim), nn.SiLU(), nn.Linear(hidden_dim, hidden_dim)
        )

        # å±žæ€§æ¡ä»¶èžåˆ MLP
        self.context_mlp = nn.Sequential(
            nn.Linear(2, hidden_dim), nn.SiLU(), nn.Linear(hidden_dim, hidden_dim)
        )

        self.egnn_layers = nn.ModuleList([EGNNLayer(hidden_dim) for _ in range(num_layers)])

        # A. åæ ‡å™ªå£°é¢„æµ‹å¤´
        self.x_noise_head = nn.Sequential(
            nn.Linear(hidden_dim, hidden_dim), nn.SiLU(), nn.Linear(hidden_dim, 3)
        )

        # B. 2D æ™¶æ ¼çº¦æŸ
        self.lattice_head = nn.Sequential(
            nn.Linear(hidden_dim, hidden_dim), nn.SiLU(), nn.Linear(hidden_dim, 9)
        )
        self.register_buffer("lattice_2d_mask", torch.tensor([[1., 1., 0.], [1., 1., 0.], [0., 0., 0.]]))

        # C. å¤šä»»åŠ¡é¢„æµ‹å¤´ (3ä¸ªåˆ†æ”¯)
        self.delta_g_head = nn.Sequential(nn.Linear(hidden_dim, 64), nn.SiLU(), nn.Linear(64, 1))
        self.stability_head = nn.Sequential(nn.Linear(hidden_dim, 64), nn.SiLU(), nn.Linear(64, 1))
        self.synthesizability_head = nn.Sequential(nn.Linear(hidden_dim, 64), nn.SiLU(), nn.Linear(64, 1))

    def forward(self, z, pos, edge_index, batch, time_step, context_delta_g=None, context_stability=None,
                p_uncond=0.15):
        # ... å‰é¢çš„ cfg_mask é€»è¾‘ä¸å˜ ...
        """
        :param p_uncond: Classifier-Free Guidance çš„æ— æ¡ä»¶ç”Ÿæˆæ¦‚çŽ‡ (é»˜è®¤ 15%)
        """
        batch_size = time_step.size(0)
        device = pos.device

        # ==========================================
        # æ¨¡å—å†…èšï¼šè‡ªåŠ¨ç”Ÿæˆ CFG æŽ©ç å¹¶ä¸¢å¼ƒæ¡ä»¶
        # ==========================================
        # ä»…åœ¨è®­ç»ƒæ¨¡å¼ (self.training) ä¸‹è¿›è¡Œéšæœº Maskï¼ŒæŽ¨æ–­æ—¶ç”±å¤–éƒ¨æ‰‹åŠ¨æŽ§åˆ¶
        if self.training and p_uncond > 0.0:
            cfg_mask = (torch.rand(batch_size, device=device) < p_uncond).float()
        else:
            cfg_mask = torch.zeros(batch_size, device=device)

        if context_delta_g is None: context_delta_g = torch.zeros(batch_size, device=device)
        if context_stability is None: context_stability = torch.zeros(batch_size, device=device)

        # å®žæ–½é®è”½ï¼šè¢« Mask çš„æ ·æœ¬ï¼Œæ¡ä»¶å˜ä¸º 0
        context_delta_g = context_delta_g * (1 - cfg_mask)
        context_stability = context_stability * (1 - cfg_mask)

        # ==========================================
        # éª¨å¹²ç½‘ç»œå‰å‘ä¼ æ’­
        # ==========================================
        h = self.node_embed(z.squeeze())
        t_emb = self.time_mlp(self.time_embed(time_step))

        ctx_cat = torch.stack([context_delta_g, context_stability], dim=-1)
        ctx_emb = self.context_mlp(ctx_cat)

        # èžåˆ æ—¶é—´+å±žæ€§ï¼Œæ³¨å…¥èŠ‚ç‚¹
        t_ctx_emb = t_emb + ctx_emb
        h = h + t_ctx_emb[batch]

        # EGNN æ¶ˆæ¯ä¼ é€’
        for layer in self.egnn_layers:
            h, pos = layer(h, pos, edge_index)

        # é¢„æµ‹åæ ‡ä¸Ž 2D æ™¶æ ¼
        eps_x = self.x_noise_head(h)
        graph_latent = scatter_mean(h, batch, dim=0)

        eps_lattice_flat = self.lattice_head(graph_latent)
        eps_lattice = eps_lattice_flat.view(-1, 3, 3) * self.lattice_2d_mask.unsqueeze(0)

        # é¢„æµ‹ä¸‰å¤§å±žæ€§
        pred_delta_g = self.delta_g_head(graph_latent).squeeze(-1)
        pred_stability = self.stability_head(graph_latent).squeeze(-1)
        pred_synth_logits = self.synthesizability_head(graph_latent).squeeze(-1)

        return {
            "eps_x": eps_x,
            "eps_lattice": eps_lattice,
            "delta_g": pred_delta_g,
            "stability": pred_stability,
            "synth_logits": pred_synth_logits,
            "cfg_mask": cfg_mask  # è¿”å›ž mask ä¾› optimization ä¸­çš„è”åˆæŸå¤±å‡½æ•°ä½¿ç”¨
        }


# ==========================================
# æµ‹è¯•ä»£ç 
# ==========================================
if __name__ == "__main__":
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

    num_nodes = 12
    batch_size = 2
    hidden_dim = 128

    z_fake = torch.randn(num_nodes, 1).to(device)  # æµ‹è¯•æ—¶æ”¹ä¸º 1 ç»´è¾“å…¥åŒ¹é… train.py
    pos_noisy = torch.randn(num_nodes, 3).to(device)
    edge_index_fake = torch.randint(0, num_nodes, (2, 30)).to(device)
    batch_fake = torch.cat([torch.zeros(6), torch.ones(6)]).long().to(device)
    time_fake = torch.randint(0, 1000, (batch_size,)).to(device)

    # æµ‹è¯•ä¼ å…¥çš„å¼•å¯¼æ¡ä»¶ (ä¾‹å¦‚ï¼Œæˆ‘ä»¬å¸Œæœ›ç”Ÿæˆ Î”G_H ä¸º 0.05 çš„å®Œç¾Žå‚¬åŒ–å‰‚)
    ctx_delta_g_fake = torch.tensor([0.05, -0.1], device=device)
    ctx_stab_fake = torch.tensor([0.0, 0.2], device=device)

    model = DenoisingEGNN(num_node_features=1, hidden_dim=hidden_dim).to(device)

    # å‰å‘ä¼ æ’­æµ‹è¯• (å¸¦æ¡ä»¶)
    outputs = model(z_fake, pos_noisy, edge_index_fake, batch_fake, time_fake,
                    context_delta_g=ctx_delta_g_fake, context_stability=ctx_stab_fake)

    print("=== Denoising EGNN (Conditional) Forward Test ===")
    print(f"åæ ‡å™ªå£°é¢„æµ‹å½¢çŠ¶ (eps_x): {outputs['eps_x'].shape}")
    print(f"æ™¶æ ¼å™ªå£°é¢„æµ‹å½¢çŠ¶ (eps_lattice): {outputs['eps_lattice'].shape}")
    print(f"å±žæ€§é¢„æµ‹è¾“å‡º (delta_g): {outputs['delta_g'].shape}")
    print("âœ… æ¨¡åž‹å·²æˆåŠŸæ”¯æŒå¸¦å±žæ€§æ¡ä»¶çš„è¾“å…¥ï¼")